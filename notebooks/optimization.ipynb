{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from loaders.LHCO import LhcoRnDLoader\n",
    "from analysis.scalar import HLFAnalysis\n",
    "from models.nn import PaeBuilder\n",
    "from utils import load_json, dump_json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "configs_folder = '../configs/analysis'\n",
    "config_files = glob.glob(configs_folder+'/*.json')\n",
    "config_files = ['../configs/configs/c3d4.json']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_files[0].replace('.', '/').split('/')[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "plots_dir = './figures/'\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.mkdir(plots_dir)\n",
    "results_history = {\n",
    " 'id': [],\n",
    " 'config': [],\n",
    " 'js_div_train': [],\n",
    " 'js_div_test': [],\n",
    " 'sig_eff': [],\n",
    " 'bkg_rej': [],\n",
    " 'auc': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for file in config_files:\n",
    "    analysis_cfg = load_json(file)\n",
    "    analysis_cfg\n",
    "\n",
    "    analysis_cfg[\"NF:n_dims\"] = analysis_cfg[\"AE:encoding_dim\"]\n",
    "    config = analysis_cfg.copy()\n",
    "\n",
    "    loader_json = analysis_cfg.pop('ANA:loader')\n",
    "    dataset_json = analysis_cfg.pop('ANA:dataset')\n",
    "    density_estimator = analysis_cfg.pop('ANA:estimator')\n",
    "    prc = analysis_cfg.pop('ANA:percentile')\n",
    "\n",
    "    loader = LhcoRnDLoader.from_json(loader_json)\n",
    "    dataset_cfg = load_json(dataset_json)\n",
    "    dataset = loader.make_full_dataset(**dataset_cfg)\n",
    "\n",
    "    builder = PaeBuilder()\n",
    "\n",
    "    pae, ae_train, nf_train = builder.from_json(analysis_cfg)\n",
    "    task = HLFAnalysis(pae, dataset=dataset)\n",
    "    task.reweighting(estimator=density_estimator, fit_key='mjj_train')\n",
    "    if 'cond' in config['nf_model']:\n",
    "        task.make_cond_inputs(['mjj_train', 'mjj_test', 'mjj_valid'])\n",
    "    task.train(ae_train,nf_train)\n",
    "    result = task.evaluate(prc = prc)\n",
    "    for key in result.keys():\n",
    "        results_history[key].append(result[key])\n",
    "    \n",
    "    id = config_files[0].replace('.', '/').split('/')[-2]\n",
    "    results_history['id'].append(id)\n",
    "    results_history['config'].append(config)\n",
    "    task.plot_training(plots_dir+id+'_train.png')\n",
    "    task.plot_latent_space(plots_dir+id+'_latent.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils import load_json, dump_json\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode\n",
    "init_notebook_mode(connected = True)\n",
    "pio.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = load_json(\"./regularization_result2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = results.pop('config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_keys = ['ANA:estimator', 'nf_model', 'AE:encoding_dim', 'AE:weight_reg', 'ANA:loader']\n",
    "cfgs_merged = {}\n",
    "for d in cfgs:\n",
    "    for k in relevant_keys: \n",
    "        cfgs_merged.setdefault(k, []).append(d[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs_merged['l1'] = [x['l1'] for x in cfgs_merged['AE:weight_reg']]\n",
    "cfgs_merged['l2'] = [x['l2'] for x in cfgs_merged['AE:weight_reg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {**results, **cfgs_merged}\n",
    "data['encoding_dim'] = data.pop('AE:encoding_dim')\n",
    "data['js_div'] = data.pop('js_div_train')\n",
    "data.pop('AE:weight_reg')\n",
    "data['density_estimator'] = data.pop('ANA:estimator')\n",
    "data['reg_l1_l2'] =['' for x in data['density_estimator']]\n",
    "data['mjj_cut'] = data['ANA:loader'] == \"../configs/loader/louis_rnd.json\"\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['mjj_cut'] = df['ANA:loader'].str.contains( '../configs/loader/louis_rnd.json')\n",
    "df['reg_l1_l2'][(df['l1'] == 1e-5) & (df['l2'] == 1e-5)] = '1e-5/1e-5'\n",
    "df['reg_l1_l2'][(df['l1'] == 1e-6) & (df['l2'] == 1e-5)] = '1e-6/1e-5'\n",
    "df['reg_l1_l2'][(df['l1'] == 1e-6) & (df['l2'] == 1e-4)] = '1e-6/1e-4'\n",
    "df['reg_l1_l2'][(df['l1'] == 1e-5) & (df['l2'] == 1e-4)] = '1e-5/1e-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df[df['mjj_cut']==False], y='auc', x='encoding_dim', size='js_div', color='reg_l1_l2', hover_data=['auc', 'js_div', 'l1', 'l2', 'id'])\n",
    "fig.update_layout(title_text='Initial Hyperparameter Scan')\n",
    "fig.write_html('results.html')\n",
    "fig.show('vscode')\n",
    "fig = px.scatter(df[df['mjj_cut']==True], y='auc', x='encoding_dim', size='js_div', color='reg_l1_l2', hover_data=['auc', 'js_div', 'l1', 'l2', 'id'])\n",
    "fig.update_layout(title_text='Initial Hyperparameter Scan')\n",
    "#fig.write_html('results.html')\n",
    "fig.show('vscode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data['l1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70f45a738872f66498ef9def3ce24bedd18274b41618f938c7df5c328e4074a6"
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
