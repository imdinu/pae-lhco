{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pae.loaders.LHCO import ScalarLoaderLHCO, DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ScalarLoaderLHCO.from_json(\"../pae/configs/loader/rnd_scalar_2j.json\")\n",
    "mjj = ScalarLoaderLHCO.from_json(\"../pae/configs/loader/rnd_scalar_mjj.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = DatasetBuilder(x, mjj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.data_preparation(sample_sizes ={'sig':1000, 'bkg': 1_000_000}, fit_key='bkg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = {'train':{'bkg':1_000_000}, 'test':{'sig':100, 'bkg': 1000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = builder.make_dataset(train = {'bkg':1_000_000}, test={'sig':100, 'bkg': 10_000}, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pae.density import GMM, ConvKDE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fftkde = ConvKDE()\n",
    "fftkde.fit(dataset[\"mjj_train\"])#, range=(1000, 9500)) \n",
    "y_kde = fftkde.evaluate(dataset[\"mjj_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ref = np.linspace(1600, 8000, 1701)\n",
    "\n",
    "y_kde = fftkde.evaluate(x_ref)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode\n",
    "init_notebook_mode(connected = True)\n",
    "pio.templates.default = \"plotly_dark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pae.models.autoencoder import DenseAutoencoder\n",
    "from pae.models.flows import MAF\n",
    "from pae.models.nn import PaeBuilder\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "\n",
    "ae_config = {\n",
    "    'input_dim':47, \n",
    "    'encoding_dim':10, \n",
    "    'units':[30, 20, 15],\n",
    "    'weight_reg':tfk.regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "    'output_activation':tf.nn.sigmoid\n",
    "}\n",
    "nf_config = {\n",
    "    'n_dims':10, \n",
    "    'n_layers':5, \n",
    "    'units':[32 for _ in range(4)]\n",
    "}\n",
    "optimizer_ae = {\n",
    "    'learning_rate': 0.001\n",
    "}\n",
    "optimizer_nf = {\n",
    "    'learning_rate': 0.005\n",
    "}\n",
    "\n",
    "builder = PaeBuilder()\n",
    "builder.make_ae_model(DenseAutoencoder, ae_config)\n",
    "builder.make_ae_optimizer(tfk.optimizers.Adam, optimizer_ae)\n",
    "builder.make_nf_model(MAF, nf_config)\n",
    "builder.make_nf_optimizer(tfk.optimizers.Adam, optimizer_nf)\n",
    "builder.compile_ae()\n",
    "builder.compile_nf()\n",
    "pae = builder.pae\n",
    "pae.ae(np.zeros(47).reshape(1,-1))\n",
    "pae.nf(np.zeros(10).reshape(1,-1))\n",
    "pae.ae.load_weights(\"./logs/full-cpu-kde-20211020-165124/ae.h5\")\n",
    "pae.nf.load_weights(\"./logs/full-cpu-kde-20211020-165124/nf.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "fold5 = KFold(8, shuffle=True)\n",
    "q= fold5.split(dataset[\"x_train\"])\n",
    "x_train, x_valid = next(q)\n",
    "print(x_train.shape)\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor \n",
    "tfd = tfp.distributions\n",
    "pae.compute_implicit_sigma(dataset['x_train'][x_valid])\n",
    "from datetime import datetime\n",
    "STEPS = 500\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "sigma = tf.constant(tf.sqrt(pae.sigma_square))\n",
    "z_ = tf.Variable(pae.ae.encoder(dataset['x_test']))\n",
    "opt = tf.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "map_summary_writer = tf.summary.create_file_writer(f\"./testing/{timestamp}/map\")\n",
    "print(z_.shape)\n",
    "\n",
    "@tf.function\n",
    "def max_apriori_prob(x, z, sigma, pae):\n",
    "    distrs = tfd.MultivariateNormalDiag(loc=x, scale_diag=sigma)\n",
    "    nf_ll = pae.nf(z)\n",
    "    reco = pae.ae.decoder(z)\n",
    "    gauss_ll = distrs.log_prob(reco)\n",
    "    #tf.print(\"gauss:\", gaussll, \"nf:\", nfll, \"\\n\")\n",
    "    return  tf.reduce_mean(-nf_ll - gauss_ll) \n",
    "\n",
    "\n",
    "@tf.function\n",
    "def find_map(x_):\n",
    "    global z_\n",
    "    if z_ is None:\n",
    "        z_ = tf.Variable(pae.ae.encoder(x_))\n",
    "    z_.assign(pae.ae.encoder(x_))\n",
    "    for i in range(STEPS):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(z_)\n",
    "            nll = max_apriori_prob(x_, z_, sigma, pae)\n",
    "        grad = tape.gradient(nll, [z_])\n",
    "        opt.apply_gradients(zip(grad, [z_]))\n",
    "        with map_summary_writer.as_default():\n",
    "            tf.summary.scalar('nll', nll, step=i)\n",
    "    return z_\n",
    "\n",
    "@tf.function\n",
    "def tf_graph_map(*args, parallel_iterations=1000):\n",
    "    return tf.map_fn(*args, parallel_iterations=parallel_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.convert_to_tensor(dataset['x_test'], dtype=tf.float32)\n",
    "\n",
    "#ds.shape\n",
    "# ds = tf.data.Dataset.from_tensor_slices(dataset['x_test'].astype(np.float32))\n",
    "# ds = ds.cache()\n",
    "#ds = ds.batch(BATCH_SIZE)\n",
    "#ds = ds.prefetch()\n",
    "#print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tf.profiler.experimental.start(f\"./testing/{timestamp}\")\n",
    "with tf.device(\"GPU:0\"):\n",
    "    # ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n",
    "    # for i, x in enumerate(ds):\n",
    "    #     ta.write(i, find_map(x))\n",
    "    # z_map = ta.concat()\n",
    "    # ta.close()\n",
    "    z_map = find_map(ds)\n",
    "tf.profiler.experimental.stop()\n",
    "#     z_map = tf_graph_map(find_map, x_test, parallel_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_map.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_map[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70f45a738872f66498ef9def3ce24bedd18274b41618f938c7df5c328e4074a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
